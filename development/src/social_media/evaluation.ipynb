{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "\n",
    "data_folder = '../../data/social_media'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to store combinations and respective scores\n",
    "\n",
    "temperature_values = [0.3, 0.7, 1.0, 1.5]\n",
    "top_p_values = [0.3, 0.6, 0.9]\n",
    "combinations = []\n",
    "\n",
    "for temperature in temperature_values:\n",
    "    for top_p in top_p_values:\n",
    "        combinations.append({\n",
    "            'temperature': temperature,\n",
    "            'top_p': top_p,\n",
    "            'total_score': 0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best values for parameters:\n",
      "- temperature = 0.3\n",
      "- top_p = 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'temperature': 0.3, 'top_p': 0.9, 'total_score': 30},\n",
       " {'temperature': 0.3, 'top_p': 0.6, 'total_score': 29},\n",
       " {'temperature': 0.7, 'top_p': 0.3, 'total_score': 29},\n",
       " {'temperature': 0.3, 'top_p': 0.3, 'total_score': 28},\n",
       " {'temperature': 1.5, 'top_p': 0.3, 'total_score': 28},\n",
       " {'temperature': 1.0, 'top_p': 0.3, 'total_score': 25},\n",
       " {'temperature': 0.7, 'top_p': 0.6, 'total_score': 24},\n",
       " {'temperature': 1.0, 'top_p': 0.6, 'total_score': 22},\n",
       " {'temperature': 1.5, 'top_p': 0.6, 'total_score': 21},\n",
       " {'temperature': 0.7, 'top_p': 0.9, 'total_score': 20},\n",
       " {'temperature': 1.0, 'top_p': 0.9, 'total_score': 17},\n",
       " {'temperature': 1.5, 'top_p': 0.9, 'total_score': 13}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum scores for each combination\n",
    "\n",
    "num_images = 20\n",
    "for index in range(num_images):\n",
    "    with open(f\"{data_folder}/results/{index}.json\", 'r') as file:\n",
    "        results = json.load(file)\n",
    "    for n, item in enumerate(results):\n",
    "        combinations[n]['total_score'] += item['score']\n",
    "\n",
    "sorted_combinations = sorted(combinations, key=lambda x: x['total_score'], reverse=True)\n",
    "\n",
    "print('Best values for parameters:')\n",
    "print(f\"- temperature = {sorted_combinations[0]['temperature']}\")\n",
    "print(f\"- top_p = {sorted_combinations[0]['top_p']}\")\n",
    "\n",
    "sorted_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# load BLIP model\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image : Image) -> str:\n",
    "    '''\n",
    "    Generates a caption for the image using the BLIP model\n",
    "    '''\n",
    "    text = 'a photograph of'\n",
    "    inputs = processor(image, text, return_tensors=\"pt\")\n",
    "    temperature = 0.3\n",
    "    top_p = 0.9\n",
    "    outputs = model.generate(**inputs, temperature=temperature, top_p=top_p, max_new_tokens=50, do_sample=True, repetition_penalty=1.2)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure image captioning times\n",
    "\n",
    "times = []\n",
    "captions = []\n",
    "\n",
    "for index in range(num_images):\n",
    "    image = Image.open(f\"{data_folder}/images/{index}.jpg\").convert('RGB')\n",
    "    start_time = time.time()\n",
    "    caption = generate_caption(image)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    times.append(elapsed_time)\n",
    "    captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 s] a photograph of a chocolate cake with two slices missing and a slice cut out\n",
      "[4.4 s] a photograph of a man sitting on top of a chair in a gym\n",
      "[3.8 s] a photograph of two people standing on a dock near the ocean\n",
      "[3.8 s] a photograph of a woman standing on a railing near a river\n",
      "[4.1 s] a photograph of a woman in black jacket and goggles on skis\n",
      "[4.0 s] a photograph of a bowl of mushroom soup with bread and parsley\n",
      "[3.7 s] a photograph of two women doing yoga in a gym room\n",
      "[3.5 s] a photograph of two children playing in a play tunnel\n",
      "[4.0 s] a photograph of a man is sitting in the cockpit of a helicopter\n",
      "[3.8 s] a photograph of two people laying on the grass near each other\n",
      "[4.1 s] a photograph of two dogs are sitting on the couch looking out the window\n",
      "[4.0 s] a photograph of a pizza in a box with pepperoni on it\n",
      "[4.6 s] a photograph of a woman standing on top of a building with a city in the background\n",
      "[3.7 s] a photograph of a blue background with a quote on it\n",
      "[3.7 s] a photograph of two men standing on the field holding trophies\n",
      "[3.8 s] a photograph of a woman wearing a white dress and straw hat\n",
      "[4.3 s] a photograph of a woman in pink and white yoga outfit doing a yoga pose\n",
      "[4.1 s] a photograph of two women sitting at a table in front of a window\n",
      "[3.8 s] a photograph of two women on skis sitting in the snow\n",
      "[4.4 s] a photograph of a woman holding a tennis racquet on top of a court\n",
      "\n",
      "Total time for 20 captions: 80.7 s\n",
      "Average time per caption: 4.0 s\n"
     ]
    }
   ],
   "source": [
    "# captions and respective times\n",
    "\n",
    "for index in range(num_images):\n",
    "    print(f\"[{round(times[index], 1)} s] {captions[index]}\")\n",
    "\n",
    "total = sum(times)\n",
    "average = total / num_images\n",
    "print(f\"\\nTotal time for 20 captions: {round(total, 1)} s\")\n",
    "print(f\"Average time per caption: {round(average, 1)} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
